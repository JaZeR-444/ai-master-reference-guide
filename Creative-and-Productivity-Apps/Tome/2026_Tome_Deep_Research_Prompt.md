# 2026 Tome Deep Research Prompt

You are a senior AI research analyst conducting a **foundational, source-driven deep research review** of Tome.

This research is for **a single, technically capable user** who uses AI tools to:
- accelerate learning and research
- build applications, scripts, and workflows
- create useful or creative software products
- explore opportunities for a future AI-powered services or product business

This analysis is **private, internal, and unsanitized**.
Assume the reader is technically literate, skeptical, and optimizing for personal leverage-not vendor marketing alignment.

Avoid hype and marketing language.

---

## RESEARCH OBJECTIVES

Produce a comprehensive, evidence-based analysis that helps determine:

- What Tome actually offers today (not aspirationally)
- Where it is genuinely strong, weak, and differentiated
- How it compares to its closest alternatives for a solo builder
- What risks, limitations, or hidden costs exist
- Whether it is worth investing **time, learning effort, and dependency** into

---

## REQUIRED SECTIONS (DO NOT SKIP)

### 1. Provider Overview & Positioning
- What problem(s) Tome is solving
- Who it is primarily built for (developers, enterprises, creatives, consumers)
- Core value proposition in plain language
- Where it sits in the broader AI ecosystem

---

### 2. Model Catalog (Complete Inventory)  **(NEW - REQUIRED)**
Produce a **complete, current list** of all models offered by Tome that are accessible via:
- API(s)
- first-party apps/products
- enterprise offerings (if distinct)

For each model, include a table with the following columns:
- Model Name (exact)
- Family / Tier (if applicable)
- Primary Modality (text/image/video/audio/multimodal/3D/etc.)
- Intended Use (as Tome positions it; paraphrase precisely and cite source)
- Key Strengths (1-3 bullets)
- Key Constraints / Known Weaknesses (1-3 bullets)
- Availability (API/app/enterprise; public vs gated; regions if relevant)
- Status (GA/Beta/Preview/Deprecated)
- Release / Last Update (if known)
- Sources (official preferred)

Rules:
- Do not guess model names.
- If the catalog is incomplete due to limited public info, state exactly what is missing and why.
- Clearly separate "current" models from "deprecated/legacy" models.

---

### 3. Core Capabilities & Modalities
- Supported modalities (text, image, video, audio, multimodal, etc.)
- Key features and technical capabilities
- How capabilities map to the models listed in the Model Catalog
- Known strengths and known gaps relevant to building and experimentation

---

### 4. Technical Architecture (High-Level)
- Publicly known or reasonably inferred architecture details
- Training approach (if disclosed)
- Inference characteristics (latency, throughput, scaling assumptions)
- Deployment model (API, hosted app, local, hybrid)

Explicitly label what is **known**, **inferred**, or **unknown**.

---

### 5. Performance, Quality, and Benchmarks
- Available benchmark results (with sources), ideally tied to specific models from the catalog
- Real-world performance observations from credible users
- Consistency, reliability, and failure modes
- Where benchmarks do or do not reflect real usage

Avoid single-number conclusions; focus on context and variance.

---

### 6. Pricing, Limits, and Economic Model
- Pricing structure and tiers, tied to specific models where applicable
- Rate limits, quotas, or usage constraints
- Cost predictability for experimentation and small-scale production
- Cost-efficiency relative to comparable alternatives

---

### 7. Safety, Policy, and Governance
- Content policies and enforcement mechanisms
- Safety systems (if disclosed)
- Constraints that materially affect experimentation or product building
- Known incidents, controversies, or regulatory exposure

---

### 8. Adoption, Ecosystem, and Real-World Usage
- Evidence of real-world adoption (especially by developers or small teams)
- Signals of production vs experimental usage
- Notable case studies or public deployments
- Gaps between claimed and observed adoption

---

### 9. Competitive Landscape
- Closest competitors (name them)
- Model-by-model comparison where appropriate (anchored to the Model Catalog)
- Clear advantages vs alternatives
- Clear disadvantages vs alternatives
- Scenarios where Tome is a poor choice for a solo builder

Be direct and critical.

---

### 10. Operational Risks, Unknowns, and Watch Areas
- Technical or platform risks
- Operational friction (outages, breaking changes, churn)
- Lock-in or migration concerns
- Unknowns that could materially impact long-term usefulness

---

### 11. Strategic Assessment (Personal Leverage Focused)
Provide a concise synthesis addressing:

- When Tome is worth investing learning time into
- When it is not worth the cognitive or dependency cost
- Best-fit use cases for solo projects, tools, or client work
- Near-term viability (0-12 months)
- Medium-term outlook (12-24 months)

Provide an overall confidence level (Low / Medium / High) and explicitly state:
- What evidence would increase this confidence
- What evidence would reduce it

---

## SOURCE REQUIREMENTS
- Prefer primary sources (official docs, model lists, pricing pages, release notes)
- Use credible third-party analysis where necessary
- Clearly distinguish fact from inference
- Cite sources inline or as a reference list

---

## OUTPUT FORMAT
- Clear section headers
- Concise but thorough
- No marketing copy
- No speculative hype
- Favor clarity over verbosity
